
# ğŸ” Random Search Algorithms for Optimization

This folder presents an in-depth comparison of **Global Random Search**, **Modified Random Search with Population-Based Sampling**, and **Gradient Descent**. These optimization methods are applied to mathematical test functions and extended to **CNN hyperparameter tuning on CIFAR-10**.

---

## ğŸ“¦ Project Files

- `random_search_algorithms.py`: Core implementations of optimization algorithms and test functions.
- `main_experiment.py`: Runs optimization on mathematical functions and generates plots.
- `cnn_hyperparameter_tuning.py`: Applies the search algorithms to CNN training.
- `hyperparameter_tuning_results.png`: Visual chart comparing performance during CNN tuning.

---

## ğŸ“Œ Highlights

- âœ… Implemented global and modified random search strategies
- âœ… Analyzed two key functions (smooth & non-differentiable)
- âœ… Compared with gradient descent analytically and visually
- âœ… Applied to CNN tuning with reproducible results

---

## ğŸ§ª Optimization Test Functions

- **Function 1:** `f(x, y) = 6(x - 1)^4 + 8(y - 2)^2`  
  - Smooth & differentiable; ideal for gradient descent.

- **Function 2:** `f(x, y) = max(x - 1, 0) + 8|y - 2|`  
  - Non-differentiable; better suited to random search.

---

## ğŸ“Š Example Chart: CNN Hyperparameter Tuning

![Hyperparameter Tuning Results](hyperparameter_tuning_results.png)

This chart shows how Global and Modified Random Search compare during the CNN tuning process.  
**Y-axis** = Negative Accuracy (lower is better), **X-axis** = Time (s)

---

## ğŸ¤– CNN Hyperparameters Tuned

- Batch size
- Learning rate
- Adam optimizerâ€™s `Î²â‚`, `Î²â‚‚`
- Number of training epochs

---

## âš™ï¸ Requirements

```bash
pip install numpy matplotlib tensorflow scikit-learn
```

---

## â–¶ï¸ How to Run

### Test Function Experiments
```bash
python main_experiment.py
```

### CNN Hyperparameter Tuning
```bash
python cnn_hyperparameter_tuning.py
```

---

## ğŸ“š Algorithms Implemented

- **Global Random Search:** Uniform random sampling
- **Modified Random Search:** Population-based adaptive sampling
- **Gradient Descent:** Derivative-based optimization for smooth landscapes

