
# Optimization Experiments Portfolio

This repository presents a set of structured experiments of **Optimization of data and algorithm Methods**. These notebooks explore different classes of optimization algorithms through theoretical and applied lenses â€” from symbolic differentiation to random search and neural network tuning.

> âš ï¸ Academic disclaimer: This repository serves solely as a personal archive and demonstration. Not intended for reuse or reproduction.

---

##  Experiments Overview

### ğŸ”¬ Experiment 1 â€“ Derivative Estimation & Gradient Descent
 *Corresponds to:* Week 2 Report  
 Folder: [`Experiment 1`](./Experiment%201/)
- Symbolic vs. numerical differentiation using `sympy` and finite differences  
- Gradient descent on `y(x) = xâ´`  
- Î´ sensitivity analysis  
- Convergence path plots and error curves

---

###  Experiment 2 â€“ Optimizer Comparison on Non-Convex Surfaces
 *Corresponds to:* Week 4 Report  
 Folder: [`Experiment 2`](./Experiment%201/)
- Polyak Step, RMSProp, Heavy Ball, and Adam  
- Contour plots and optimization path visualizations  
- Non-smooth function behavior analysis  
- Comparison of optimizer robustness

---

###  Experiment 3 â€“ Loss Surface Navigation & Step Size Sensitivity
 *Corresponds to:* Week 6 Report  
Folder: [`Experiment 3`](./Experiment%201/)
- Advanced optimizers applied to non-convex surfaces  
- Mini-batch stochastic gradient descent  
- Step size & batch size effect on convergence  
- Hessian analysis and 3D surface interpretation

---

### ğŸ¤– Experiment 4 â€“ Random Search for CNN Hyperparameter Tuning
 *Corresponds to:* Week 8 Report  
Folder: [`Experiment 4`](./Experiment%201/)
- Global and modified random search  
- Application to CIFAR-10 CNN training  
- Time vs. cost vs. accuracy comparisons  
- Hyperparameter tuning efficiency chart

---

## Author

**Vishaalini Ramasamy Manikandan**  
ğŸ“§ [vishaalini70@gmail.com](mailto:vishaalini70@gmail.com)  
ğŸ”— [LinkedIn](https://www.linkedin.com/in/vishaalini-manikandan/)

---

ğŸ“Œ *This work reflects my learning journey through optimization and applied ML.*  

