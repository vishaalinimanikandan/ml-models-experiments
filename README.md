# ml-models-experiments
A collection of machine learning model experiments focused on supervised and unsupervised learning, model evaluation, tuning, and real-world datasets.
# ðŸ“ˆ Derivative Estimation & Gradient Descent in Python

This project explores the optimization of mathematical functions using both symbolic and numerical methods in Python. It implements derivative calculation using the `sympy` library and compares it against numerical estimation techniques like the finite difference method. Additionally, it demonstrates how gradient descent behaves under different step sizes, initial values, and function parameters.

---

## ðŸ“Œ Topics Covered

- Symbolic Derivative Calculation (`sympy`)
- Numerical Derivative Estimation using Finite Differences
- Impact of Perturbation Size (Î´) on Accuracy
- Implementation of Gradient Descent
- Analysis of Step Size (Î±) and Convergence
- Parameter Sensitivity with Î³ in optimization functions

---

## ðŸ“Š Key Visualizations


- **Figure 5**: Final x Values After 50 Iterations for Varying Î±
![image](https://github.com/user-attachments/assets/2d0f99a6-796c-4cd7-a0b9-b56ffc0f7420)


## ðŸ›  Technologies Used

- Python (NumPy, sympy, matplotlib)
- Jupyter Notebook
- Gradient Descent Algorithms
- Finite Difference Method

---

