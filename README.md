
# Optimization Experiments Portfolio

This repository presents a set of structured experiments of **Optimization of data and algorithm Methods**. These notebooks explore different classes of optimization algorithms through theoretical and applied lenses — from symbolic differentiation to random search and neural network tuning.

> ⚠️ Academic disclaimer: This repository serves solely as a personal archive and demonstration. Not intended for reuse or reproduction.

---

##  Experiments Overview

### 🔬 Experiment 1 – Derivative Estimation & Gradient Descent
 *Corresponds to:* Week 2 Report  
 Folder: [`Experiment 1`](./Experiment%201/)
- Symbolic vs. numerical differentiation using `sympy` and finite differences  
- Gradient descent on `y(x) = x⁴`  
- δ sensitivity analysis  
- Convergence path plots and error curves

---

###  Experiment 2 – Optimizer Comparison on Non-Convex Surfaces
 *Corresponds to:* Week 4 Report  
 Folder: [`Experiment 2`](./Experiment%201/)
- Polyak Step, RMSProp, Heavy Ball, and Adam  
- Contour plots and optimization path visualizations  
- Non-smooth function behavior analysis  
- Comparison of optimizer robustness

---

###  Experiment 3 – Loss Surface Navigation & Step Size Sensitivity
 *Corresponds to:* Week 6 Report  
Folder: [`Experiment 3`](./Experiment%201/)
- Advanced optimizers applied to non-convex surfaces  
- Mini-batch stochastic gradient descent  
- Step size & batch size effect on convergence  
- Hessian analysis and 3D surface interpretation

---

### 🤖 Experiment 4 – Random Search for CNN Hyperparameter Tuning
 *Corresponds to:* Week 8 Report  
Folder: [`Experiment 4`](./Experiment%201/)
- Global and modified random search  
- Application to CIFAR-10 CNN training  
- Time vs. cost vs. accuracy comparisons  
- Hyperparameter tuning efficiency chart

---

## Author

**Vishaalini Ramasamy Manikandan**  
📧 [vishaalini70@gmail.com](mailto:vishaalini70@gmail.com)  
🔗 [LinkedIn](https://www.linkedin.com/in/vishaalini-manikandan/)

---

📌 *This work reflects my learning journey through optimization and applied ML.*  

